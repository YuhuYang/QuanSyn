{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###indicators form dependency grammar and valency grammar,such as dependency distance, dependency direction, \n",
    "###probabilistic valency pattern, valency and so on.###\n",
    "from conllu import parse_incr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyAnalyzer():\n",
    "    \"\"\"\n",
    "    An analyzer for dependency.\n",
    "    \n",
    "    :Items: mean dependency distance(mdd), dependency distance distribution(dd_distribution),\n",
    "            proportion of dependency directions(pdd), \n",
    "            mean hierarchical distance(mhd), hierarchical distance distribution(hd_distribution),\n",
    "            tree height and tree width(tree),\n",
    "            describe(mdd,mhd,pdd,tree_hei,tree_wid,vk)\n",
    "    \"\"\"\n",
    "    def __init__(self,data):\n",
    "        \"\"\"\n",
    "        :data: must be conllu format or other byte-like formats, which means annotated\n",
    "               such as, f = open(r'treebank.conllu',encoding='utf-8').read()\n",
    "                        DenpendencyAnalyzer(f)\n",
    "        \"\"\"\n",
    "        self.data = list(parse_incr(data))\n",
    "        \n",
    "    def _pos_condition(self,word, pos):\n",
    "        return word['upos'] == pos\n",
    "    def _dep_condition(self,word, dep):\n",
    "        return word['deprel'] == dep\n",
    "    def _dir_condition(self,word, direc):\n",
    "        if direc == 'hi':\n",
    "            return word['id'] - word['head'] < 0\n",
    "        if direc == 'hf':\n",
    "            return word['id'] - word['head'] > 0\n",
    "    def select_conditions(self,word, pos=None, dependency=None, direc=None):\n",
    "        conditions = []\n",
    "        conditions.append(True)\n",
    "        if pos:\n",
    "            conditions.append(self._pos_condition(word, pos))\n",
    "        if dependency:\n",
    "            conditions.append(self._dep_condition(word, dependency))\n",
    "        if direc:\n",
    "            conditions.append(self._dep_condition(word, direc))\n",
    "        return conditions\n",
    "    \n",
    "    def mdd(self,pos=None,dependency=None,direction=None):\n",
    "        \"\"\"\n",
    "        These three paras can specialize the items you want.\n",
    "        :pos: str\n",
    "        :dependency: str\n",
    "        :direction: str\n",
    "                    'hi' - head final\n",
    "                    'hf' - hean initial\n",
    "        :return: mdd(float)\n",
    "        :About mdd: \n",
    "               Liu H. Dependency distance as a metric of language comprehension difficulty[J]. \n",
    "                 Journal of Cognitive Science, 2008, 9(2): 159-191.  \n",
    "        :example:\n",
    "              dep = DenpendencyAnalyzer(conllu)\n",
    "              dep.mdd()\n",
    "              dep.mdd(pos='NOUN')\n",
    "        \"\"\"\n",
    "        dds = self.dd_distribution(pos,dependency,direction)\n",
    "        MDD = sum(dds) / len(dds)\n",
    "        return MDD    \n",
    "    def dd_distribution(self,pos=None,dependency=None,direction=None):\n",
    "        dds = []\n",
    "        for sentence in self.data:    \n",
    "            for word in sentence:\n",
    "                conditions = self.select_conditions(word, pos, dependency, direction)\n",
    "                if all(conditions+[word['deprel'] not in ['root','punct']]):\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "        return dds      \n",
    "    \n",
    "    def pdd(self,pos=None,dependency=None):\n",
    "        \"\"\"\n",
    "        :return: dict -> (proportion of head final, proportion of head initial)\n",
    "        :About dependency direction:\n",
    "              Liu-Directionalities\n",
    "              Liu H. Dependency direction as a means of word-order typology: A method based on dependency treebanks[J].\n",
    "                Lingua, 2010, 120(6): 1567-1578.\n",
    "        :example:\n",
    "              dep = DenpendencyAnalyzer(conllu)\n",
    "              dep.pdd()\n",
    "        \n",
    "        \"\"\"\n",
    "        head_initial = 0\n",
    "        head_final = 0\n",
    "        for sentence in self.data:    \n",
    "            for word in sentence:\n",
    "                conditions = self.select_conditions(word, pos, dependency)\n",
    "                if all(conditions+[word['deprel'] not in ['root','punct']]):\n",
    "                    if word['head'] < word['id']:\n",
    "                        head_initial += 1\n",
    "                    else:\n",
    "                        head_final += 1\n",
    "                        \n",
    "        total = head_initial + head_final\n",
    "        proportion_head_initial = head_initial / total\n",
    "        proportion_head_final = head_final / total\n",
    "        pdd = {'head final':proportion_head_final,'head_initial':proportion_head_initial}\n",
    "        return  pdd\n",
    "    \n",
    "    def mhd(self, pos=None,dependency=None):\n",
    "        \"\"\"\n",
    "        About mean hierarchical distance:\n",
    "             Jing Y, Liu H. Mean hierarchical distance augmenting mean dependency distance[C]//\n",
    "               Proceedings of the third international conference on dependency linguistics \n",
    "               (Depling 2015). 2015: 161-170.\n",
    "        \"\"\"\n",
    "        hds = self.hd_distribution(pos,dependency)\n",
    "        MHD = sum(hds)/len(hds)\n",
    "        return MHD\n",
    "    def hd_distribution(self, pos=None,dependency=None):\n",
    "        hds = []\n",
    "        for sentence in self.data:\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            selected_words = [word for word in sentence if all(self.select_conditions(word, pos, dependency)+[word['deprel'] not in ['root','punct']])]\n",
    "            for word in selected_words:            \n",
    "                head_id = word['head']\n",
    "                distance = 0\n",
    "                while head_id != 0:\n",
    "                    distance += 1\n",
    "                    head_id = word_index_by_id[head_id]['head']\n",
    "                hds.append(distance)\n",
    "        return hds    \n",
    "\n",
    "    def tree(self,pos=None,dependency=None):        \n",
    "        \"\"\"\n",
    "        About tree height and tree width:\n",
    "             Hongxin Zhang & Haitao Liu. (2018). Interrelations among dependency tree widths, \n",
    "               heights and sentence lengths. In: Haitao Liu & Jingyang Jiang (eds.). \n",
    "               Quantitative Analysis of Dependency Structure. Berlin/Boston: DE GRUYTER MOUTON.\n",
    "        \"\"\"\n",
    "        tree_indicators = self.tree_distribution(pos,dependency)\n",
    "        tree_height = tree_indicators['height']\n",
    "        tree_width = tree_indicators['width']\n",
    "        tree = {'height':sum(tree_height)/len(tree_height),'width':sum(tree_width)/len(tree_width)}\n",
    "        return tree\n",
    "    def tree_distribution(self,pos=None,dependency=None):\n",
    "        height = []\n",
    "        width = []\n",
    "        for sentence in self.data:\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            levels = []\n",
    "            for word in sentence:\n",
    "                conditions = self.select_conditions(word, pos, dependency)\n",
    "                if all(conditions+[word['deprel'] != 'punct']):\n",
    "                    depth = 0        \n",
    "                    head_id = word['head']\n",
    "                    while head_id != 0:\n",
    "                        depth += 1\n",
    "                        head_id = word_index_by_id[head_id]['head']\n",
    "                    levels.append(depth)\n",
    "            height.append(max(levels))\n",
    "            width.append(levels.count(max(levels, key=levels.count)))\n",
    "        tree = {'height':height,'width':width}\n",
    "        return tree\n",
    "    \n",
    "    def describe(self,target='text'):\n",
    "        sents_info = []\n",
    "        for sentence in self.data:\n",
    "            sent_data = {}\n",
    "            dds = []\n",
    "            levels = []\n",
    "            valencies = []\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            for word in sentence:\n",
    "                if word['deprel'] != 'punct':\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                valencies.append(len([i for i in sentence if i['head'] == word['id']]))\n",
    "                depth = 0        \n",
    "                head_id = word['head']\n",
    "                while head_id != 0:\n",
    "                    depth += 1\n",
    "                    head_id = word_index_by_id[head_id]['head']\n",
    "                levels.append(depth)\n",
    "            mdd = sum(dds) / len(dds)\n",
    "            sent_length = len(levels)\n",
    "            vk = (sum(i*2 for i in valencies)/sent_length) - (2 - 2/sent_length)**2\n",
    "            sent_data['mdd'] = mdd\n",
    "            sent_data['mhd'] = sum(levels)/len(levels)\n",
    "            sent_data['sent_length'] = sent_length\n",
    "            sent_data['tree_height'] = max(levels)\n",
    "            sent_data['tree_width'] = levels.count(max(levels, key=levels.count))\n",
    "            sent_data['vk'] = vk\n",
    "            sents_info.append(sent_data)\n",
    "        \n",
    "        if target=='text':\n",
    "            mdd = sum([i['mdd'] for i in sents_info])/len(sents_info)\n",
    "            mhd = sum([i['mhd'] for i in sents_info])/len(sents_info) \n",
    "            senlen = sum([i['sent_length'] for i in sents_info])/len(sents_info)\n",
    "            tree_hei = sum([i['tree_height'] for i in sents_info])/len(sents_info)\n",
    "            tree_wid = sum([i['tree_width'] for i in sents_info])/len(sents_info)\n",
    "            vk = sum([i['vk'] for i in sents_info])/len(sents_info) \n",
    "            text_info = {'mdd':mdd,'mhd':mhd,'sent_length':senlen,'tree_height':tree_hei,\n",
    "                        'tree_width':tree_wid,'vk':vk}\n",
    "            return text_info \n",
    "        \n",
    "        if target=='sentence':\n",
    "            return sents_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValencyAnalyzer():\n",
    "    \"\"\"\n",
    "    A class for analyzing valency.\n",
    "    \n",
    "    :data: must be conllu format or other byte-like formats, which means annotated corpus(treebanks). \n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = list(parse_incr(data))\n",
    "\n",
    "    def _pos_condition(self,word, pos):\n",
    "        return word['upos'] == pos\n",
    "    def _dep_condition(self,word, dep):\n",
    "        return word['deprel'] == dep\n",
    "    def select_conditions(self,word, pos=None, dependency=None):\n",
    "        conditions = []\n",
    "        conditions.append(True)\n",
    "        if pos:\n",
    "            conditions.append(self._pos_condition(word, pos))\n",
    "        if dependency:\n",
    "            conditions.append(self._dep_condition(word, dependency))\n",
    "        return conditions    \n",
    "    \n",
    "    def mean_valency(self,pos=None):\n",
    "        val_distribution = self.distribution(pos)\n",
    "        mean_valency = sum(val_distribution)/len(val_distribution)\n",
    "        return mean_valency\n",
    "\n",
    "    def distribution(self,pos=None):\n",
    "        val = []\n",
    "        for sentence in self.data:    \n",
    "            for word in sentence:\n",
    "                if all(self.select_conditions(word, pos)+[word['deprel'] != 'punct']): \n",
    "                    depdents = [w for w in sentence if w['head'] == word['id']]\n",
    "                    val.append(len(depdents)) \n",
    "        return val\n",
    "    \n",
    "    def PVP(self,pos=None,target='dependency'):\n",
    "        dependents = []\n",
    "        governors = []\n",
    "        for sentence in self.data:    \n",
    "            for word in sentence:\n",
    "                conditions = self.select_conditions(word, pos)\n",
    "                if all(conditions+[target=='dependency']): \n",
    "                    dependent = [w['deprel'] for w in sentence if w['head'] == word['id']]\n",
    "                    dependents += dependent\n",
    "                    governor = word['deprel']\n",
    "                    governors.append(governor)\n",
    "                if all(conditions+[target=='wordclass']):\n",
    "                    dependent = [w['upos'] for w in sentence if w['head'] == word['id']]\n",
    "                    dependents += dependent\n",
    "                    governor = [w['upos'] for w in sentence if w['id'] == word['head']]\n",
    "                    governors += governor                   \n",
    "                    \n",
    "        deps = Counter(dependents)\n",
    "        govs = Counter(governors)\n",
    "        pvp = {'act as a gov':deps,'act as a dep':govs}\n",
    "        return pvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act as a gov': Counter({'NOUN': 895, 'PUNCT': 803, 'ADV': 673, 'VERB': 636, 'PRON': 541, 'PART': 241, 'ADP': 212, 'AUX': 127, 'ADJ': 71, 'PROPN': 61, 'CCONJ': 39, 'SCONJ': 28, 'INTJ': 13, 'DET': 5, 'NUM': 4}), 'act as a dep': Counter({'VERB': 636, 'AUX': 340, 'NOUN': 98, 'PART': 80, 'SCONJ': 51, 'ADP': 41, 'ADJ': 17, 'ADV': 12, 'PRON': 3, 'DET': 2})}\n",
      "Wall time: 244 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = open(r'D:\\database\\sud-treebanks-v2.12\\SUD_Chinese-HK\\zh_hk-sud-test.conllu',\"r\",encoding=\"utf-8\")\n",
    "d = ValencyAnalyzer(data)\n",
    "print(d.PVP(pos='VERB',target='wordclass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
