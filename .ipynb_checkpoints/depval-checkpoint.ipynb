{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###indicators form dependency grammar and valency grammar,such as dependency distance, dependency direction, \n",
    "###probabilistic valency pattern, valency and so on.###\n",
    "from conllu import parse_incr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyAnalyzer():\n",
    "    \"\"\"\n",
    "    An analyzer for dependency.\n",
    "    \n",
    "    :Items: mean dependency distance, dependency direction, dependency distribution\n",
    "    \"\"\"\n",
    "    def __init__(self,data):\n",
    "        \"\"\"\n",
    "        :data: must be conllu format or other byte-like formats, which means annotated\n",
    "        \"\"\"\n",
    "        self.data= data\n",
    "    \n",
    "    def mdd(self,upos=None,dorg=None, deprel=None, dependency_direction=None):\n",
    "        \"\"\"\n",
    "        :upos: None(default) - count the mdd of all words without distinguishing POS.\n",
    "               POS(str) - like 'NOUN','VERB'. Count the mdd of words with the POS \n",
    "               tags you defined.\n",
    "        :dorg:dependent or governor(str). Must use it along with upos being activated. \n",
    "              'dep'- count the mdd of words when acting as a depdent\n",
    "              'gov'- count the mdd of words when acting as a governor\n",
    "        :deprel: dependency relations(str), such as 'subj'. When setting it to a specific\n",
    "                 deprel, the result will be the mdd of that deprel.\n",
    "        :dependency_direction: str\n",
    "                               'head_initial' - \n",
    "                               'head_final' - \n",
    "        :return: mdd(float)\n",
    "        :About mdd: \n",
    "               Liu H. Dependency distance as a metric of language comprehension difficulty[J]. \n",
    "               Journal of Cognitive Science, 2008, 9(2): 159-191.        \n",
    "        \"\"\"\n",
    "        dds = []\n",
    "        for sentence in parse_incr(self.data):    \n",
    "            for word in sentence:\n",
    "                if word['upos'] == upos and word['head'] !=0 and dorg == 'dep':\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif word['upos'] == upos and dorg == 'gov':\n",
    "                    dd = [abs(word['id']-i['id']) for i in sentence if i['head'] == word['id'] and i['deprel'] != 'punct']\n",
    "                    dds = dds + dd\n",
    "                elif deprel is not None and word['deprel'] == deprel:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif dependency_direction == 'head_final' and word['deprel']!='punct'and word['head']!=0 and word['head'] > word['id']:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif dependency_direction == 'head_initial' and word['deprel'] !='punct'and word['head']!=0 and word['head'] < word['id']:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif deprel is None and upos is None and dependency_direction is None and word['deprel']!='punct'and word['head']!=0:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "        MDD = sum(dds) / len(dds)\n",
    "        return MDD\n",
    "    \n",
    "    def HF_HI(self,upos=None,deprel=None):\n",
    "        \"\"\"\n",
    "        :return: list \n",
    "        :About dependency direction:\n",
    "              Liu H. Dependency direction as a means of word-order typology: A method based on dependency treebanks[J].\n",
    "              Lingua, 2010, 120(6): 1567-1578.\n",
    "        \n",
    "        \"\"\"\n",
    "        head_initial = 0\n",
    "        head_final = 0\n",
    "        for sentence in parse_incr(self.data):    \n",
    "            for word in sentence:\n",
    "                if word['deprel'] == deprel:\n",
    "                    if word['head'] < word['id']:\n",
    "                        head_initial += 1\n",
    "                    else:\n",
    "                        head_final += 1\n",
    "                elif word['upos'] == upos and word['head']!=0:\n",
    "                    if word['head'] < word['id']:\n",
    "                        head_initial += 1\n",
    "                    else:\n",
    "                        head_final += 1\n",
    "                elif deprel is None and upos is None and word['deprel']!='punct'and word['head']!=0:\n",
    "                    if word['head'] < word['id']:\n",
    "                        head_initial += 1\n",
    "                    else:\n",
    "                        head_final += 1\n",
    "                        \n",
    "        total = head_initial + head_final\n",
    "        proportion_head_initial = head_initial / total\n",
    "        proportion_head_final = head_final / total\n",
    "\n",
    "        return  proportion_head_final,proportion_head_initial\n",
    "    \n",
    "    def dd_distribution(self, upos=None,deprel=None,dependency_direction=None,dorg=None):\n",
    "        \"\"\"\n",
    "        :return: list\n",
    "        \"\"\"\n",
    "        dds = []\n",
    "        for sentence in parse_incr(self.data):    \n",
    "            for word in sentence:\n",
    "                if word['upos'] == upos and word['head'] !=0 and dorg == 'dep':\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif word['upos'] == upos and dorg == 'gov':\n",
    "                    dd = [abs(word['id']-i['id']) for i in sentence if i['head'] == word['id'] and i['deprel'] != 'punct']\n",
    "                    dds = dds + dd\n",
    "                elif deprel is not None and word['deprel'] == deprel:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif dependency_direction == 'head_final' and word['deprel']!='punct'and word['head']!=0 and word['head'] > word['id']:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif dependency_direction == 'head_initial' and word['deprel'] !='punct'and word['head']!=0 and word['head'] < word['id']:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "                elif deprel is None and upos is None and dependency_direction is None and word['deprel']!='punct'and word['head']!=0:\n",
    "                    dd = abs(word['head'] - word['id'])\n",
    "                    dds.append(dd)\n",
    "        return dds\n",
    "    \n",
    "    def mhd(self, upos=None,deprel=None):\n",
    "        \"\"\"\n",
    "        About mean hierarchical distance:\n",
    "             Jing Y, Liu H. Mean hierarchical distance augmenting mean dependency distance[C]//\n",
    "             Proceedings of the third international conference on dependency linguistics \n",
    "             (Depling 2015). 2015: 161-170.\n",
    "        \"\"\"\n",
    "        total_hd = 0\n",
    "        total_count = 0\n",
    "        for sentence in parse_incr(self.data):\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            if upos is not None:\n",
    "                selected_words = [word for word in sentence if upos is None or word['upos'] == upos]\n",
    "            elif deprel is not None:\n",
    "                selected_words = [word for word in sentence if upos is None or word['deprel'] == deprel]\n",
    "            elif upos is None and deprel is None:\n",
    "                selected_words = [word for word in sentence]\n",
    "            for word in selected_words:\n",
    "                head_id = word['head']\n",
    "                distance = 0\n",
    "                while head_id != 0:\n",
    "                    distance += 1\n",
    "                    head_id = word_index_by_id[head_id]['head']\n",
    "                total_hd += distance\n",
    "                total_count += 1\n",
    "        MHD = total_hd / total_count\n",
    "        return MHD\n",
    "    \n",
    "    def hd_distribution(self,upos=None,deprel=None):\n",
    "        distance_distribution = []\n",
    "\n",
    "        for sentence in parse_incr(self.data):\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            if upos is not None:\n",
    "                selected_words = [word for word in sentence if upos is None or word['upos'] == upos]\n",
    "            elif deprel is not None:\n",
    "                selected_words = [word for word in sentence if upos is None or word['deprel'] == deprel]\n",
    "            elif upos is None and deprel is None:\n",
    "                selected_words = [word for word in sentence]\n",
    "            for word in selected_words:\n",
    "                head_id = word['head']\n",
    "                distance = 0\n",
    "                while head_id != 0:\n",
    "                    distance += 1\n",
    "                    head_id = word_index_by_id[head_id]['head']\n",
    "                distance_distribution.append(distance)\n",
    "        return distance_distribution\n",
    "    \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        :return: [{},{}]\n",
    "        :About tree height and three width:\n",
    "                Zhang H, Liu H. Interrelations among dependency tree widths, heights and sentence lengths[J]. \n",
    "                Quantitative Analysis of Dependency Structures, 2018, 72: 31-52.\n",
    "        :About vk:\n",
    "                Lu Q, Lin Y, Liu H. Dynamic Valency and Dependency Distance[J]. Quantitative Analysis of \n",
    "                Dependency Structures, 2018, 72: 145.\n",
    "        \n",
    "        \"\"\"\n",
    "        table = []\n",
    "        for sentence in parse_incr(self.data):\n",
    "            sent_data = {}\n",
    "            dds = []\n",
    "            word_index_by_id = {word['id']: word for word in sentence}\n",
    "            levels = []\n",
    "            valencies = []\n",
    "            for word in sentence:\n",
    "                if word['deprel'] != 'punct':\n",
    "                    if word['head'] !=0:\n",
    "                        dd = abs(word['head'] - word['id'])\n",
    "                        dds.append(dd)\n",
    "                        valency = len([i for i in sentence if i['head'] == word['id']])+1\n",
    "                        valencies.append(valency)\n",
    "                    if word['head'] ==0:\n",
    "                        valency = len([i for i in sentence if i['head'] == word['id']])\n",
    "                        valencies.append(valency)\n",
    "                    depth = 0        \n",
    "                    head_id = word['head']\n",
    "                    while head_id != 0:\n",
    "                        depth += 1\n",
    "                        head_id = word_index_by_id[head_id]['head']\n",
    "                    levels.append(depth)\n",
    "                    \n",
    "        \n",
    "            MDD = sum(dds) / len(dds)\n",
    "            sent_length = len(levels)\n",
    "            vk = (sum(i*2 for i in valencies)/sent_length) - (2 - 2/sent_length)**2\n",
    "            sent_data['mdd'] = MDD\n",
    "            sent_data['mhd'] = sum(levels)/len(levels)\n",
    "            sent_data['sent_length'] = sent_length\n",
    "            sent_data['tree_height'] = max(levels)\n",
    "            sent_data['tree_width'] = levels.count(max(levels, key=levels.count))\n",
    "            sent_data['vk'] = vk\n",
    "        table.append(sent_data)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mdd': 3.6666666666666665,\n",
       "  'mhd': 2.2,\n",
       "  'sent_length': 10,\n",
       "  'tree_height': 5,\n",
       "  'tree_width': 3,\n",
       "  'vk': 0.5599999999999996}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = open(r'D:\\database\\sud-treebanks-v2.9\\SUD_Chinese-GSDSimp\\演示.conllu',\"r\",encoding=\"utf-8\")\n",
    "d = DependencyAnalyzer(data)\n",
    "d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValencyAnalyzer():\n",
    "    \"\"\"\n",
    "    A class for analyzing valency.\n",
    "    \n",
    "    :data: must be conllu format or other byte-like formats, which means annotated corpus(treebanks). \n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "            \n",
    "    def mean_valency(self,upos=None):\n",
    "        \"\"\"\n",
    "        :upos: None(default) - count the mean valency of all words without distinguishing POS.\n",
    "           POS(str) - like 'NOUN','VERB'. Count the mean valency of words with the POS \n",
    "           tags you defined.\n",
    "        :return: mean valency(float)\n",
    "        :About mean valency: \n",
    "           Yan J, Liu H. Quantitative analysis of Chinese and English verb valencies \n",
    "           based on probabilistic valency pattern theory[C]//Workshop on Chinese Lexical\n",
    "           Semantics. Cham: Springer International Publishing, 2021: 152-162. \n",
    "        \"\"\"   \n",
    "        if upos != None:\n",
    "            num_depdents = 0\n",
    "            num_word = 0\n",
    "            for sentence in parse_incr(self.data):    \n",
    "                for word in sentence:\n",
    "                    if word['upos'] == upos: \n",
    "                        depdents = [w for w in sentence if w['head'] == word['id']]\n",
    "                        num_depdents += len(depdents)\n",
    "                        num_word += 1\n",
    "            mean_valency = num_depdents/num_word\n",
    "            return mean_valency\n",
    "        \n",
    "        elif upos == None:\n",
    "            num_depdents = 0\n",
    "            num_word = 0\n",
    "            for sentence in parse_incr(self.data):    \n",
    "                for word in sentence:\n",
    "                    if word['deprel'] != 'punct': \n",
    "                        depdents = [w for w in sentence if w['head'] == word['id']]\n",
    "                        num_depdents += len(depdents)\n",
    "                        num_word += 1\n",
    "            mean_valency = num_depdents/num_word\n",
    "            return mean_valency\n",
    "    \n",
    "    def distribution(self, upos=None):\n",
    "        '''\n",
    "        :upos: None(default) - count the dynamic valency distribution of all words without distinguishing POS.\n",
    "               POS(str) - like 'NOUN','VERB'. Count the dynamic valency distribution of words with the POS \n",
    "               tags you defined.\n",
    "        :return: valency distribution(list)\n",
    "        :About valency distribution:\n",
    "               Yan J, Liu H. Quantitative analysis of Chinese and English verb valencies \n",
    "               based on probabilistic valency pattern theory[C]//Workshop on Chinese Lexical\n",
    "               Semantics. Cham: Springer International Publishing, 2021: 152-162. \n",
    "        '''\n",
    "        if upos is not None:\n",
    "            v = []\n",
    "            for sentence in parse_incr(self.data):    \n",
    "                for word in sentence:\n",
    "                    if word['upos'] == upos: \n",
    "                        dependents = [w for w in sentence if w['head'] == word['id']]\n",
    "                        v.append(len(dependents))\n",
    "            return v\n",
    "        \n",
    "        elif upos is None:\n",
    "            v = []\n",
    "            for sentence in parse_incr(self.data):    \n",
    "                for word in sentence:\n",
    "                    if word['deprel'] != 'punct': \n",
    "                        dependents = [w for w in sentence if w['head'] == word['id']]\n",
    "                        v.append(len(dependents))\n",
    "            return v    \n",
    "    \n",
    "    def PVP(self,upos=None,target='deprel'):\n",
    "        '''\n",
    "        PVP - probabilistic valency pattern theory, a kind of generalize valency theory. PVP can be used\n",
    "          for describing word's/wordclass's capacity to combine with others.\n",
    "    \n",
    "        :upos: None(default) - count the PVP of all words without distinguishingdistincting POS.\n",
    "           POS(str) - like 'NOUN','VERB'. Count the PVP of words with the POS tags you defined.\n",
    "        :target: the feature you want to count\n",
    "           'deprel'(default) - dependency relations\n",
    "           'upos' - POS\n",
    "        :return: PVP_dependents(dict),PVP_governors(dict)\n",
    "        :About PVP:\n",
    "           刘海涛,冯志伟.自然语言处理的概率配价模式理论[J].语言科学,2007(03):32-41.\n",
    "           Liu, H. & Feng, Z. 2007. Probabilistic Valency Pattern Theory for Natural Language Processing.\n",
    "           Language Science(03):32-41.\n",
    "\n",
    "        '''\n",
    "        if target == 'deprel':\n",
    "            if upos != None:\n",
    "                dependents = []\n",
    "                governors = []\n",
    "                for sentence in parse_incr(self.data):    \n",
    "                    for word in sentence:\n",
    "                        if word['upos'] == upos: \n",
    "                            dependent = [w['deprel'] for w in sentence if w['head'] == word['id']]\n",
    "                            dependents += dependent\n",
    "                            governor = word['deprel']\n",
    "                            governors.append(governor)\n",
    "                deps = Counter(dependents)\n",
    "                govs = Counter(governors)\n",
    "                return deps, govs\n",
    "        \n",
    "            if upos == None:\n",
    "                dependents = []\n",
    "                governors = []\n",
    "                for sentence in parse_incr(self.data):    \n",
    "                    for word in sentence:\n",
    "                        if word['deprel'] != 'punct': \n",
    "                            dependent = [w['deprel'] for w in sentence if w['head'] == word['id']]\n",
    "                            dependents += dependent\n",
    "                            governor = word['deprel']\n",
    "                            governors.append(governor)\n",
    "                deps = Counter(dependents)\n",
    "                govs = Counter(governors)\n",
    "                return deps, govs\n",
    "            \n",
    "        if target == 'upos':\n",
    "            if upos != None:\n",
    "                dependents = []\n",
    "                governors = []\n",
    "                for sentence in parse_incr(self.data):    \n",
    "                    for word in sentence:\n",
    "                        if word['upos'] == upos: \n",
    "                            dependent = [w['upos'] for w in sentence if w['head'] == word['id']]\n",
    "                            dependents += dependent\n",
    "                            governor = word['upos']\n",
    "                            governors.append(governor)\n",
    "                deps = Counter(dependents)\n",
    "                govs = Counter(governors)\n",
    "                return deps, govs\n",
    "        \n",
    "            if upos == None:\n",
    "                dependents = []\n",
    "                governors = []\n",
    "                for sentence in parse_incr(self.data):    \n",
    "                    for word in sentence:\n",
    "                        if word['deprel'] != 'punct': \n",
    "                            dependent = [w['upos'] for w in sentence if w['head'] == word['id']]\n",
    "                            dependents += dependent\n",
    "                            governor = word['upos']\n",
    "                            governors.append(governor)\n",
    "                deps = Counter(dependents)\n",
    "                govs = Counter(governors)\n",
    "                return deps, govs\n",
    "            \n",
    "    #prepare for non-generalized valency         \n",
    "    #def complement_selecter(data,name='ud'): \n",
    "    #    if name == 'ud':\n",
    "    #        #Universal Dependenecies\n",
    "    #        complements = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
